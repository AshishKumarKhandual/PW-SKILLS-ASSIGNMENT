{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcc887d",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**What is Deep Learning? Briefly describe how it evolved and how it differs from traditional machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2c646",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Deep Learning is a subset of Machine Learning that uses multi-layered neural networks to automatically learn complex data representations. It evolved from early perceptron models in the 1950s, expanded with backpropagation in the 1980s, and has grown significantly in recent decades due to advances in computing power (GPUs), availability of large datasets, and improved algorithms. Unlike traditional ML, which relies on handcrafted features, Deep Learning automatically extracts hierarchical features directly from raw data, enabling superior performance in vision, speech, and NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d85fc",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "**Explain the basic architecture and functioning of a Perceptron. What are its limitations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8c3ae",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A Perceptron consists of input nodes, associated weights, a summation function, and an activation function. Inputs are multiplied by their respective weights, summed, and passed through an activation function to produce an output. Limitations include inability to solve non-linearly separable problems (like XOR), sensitivity to input scaling, and being limited to linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38b511",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "**Describe the purpose of activation function in neural networks. Compare Sigmoid, ReLU, and Tanh functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68adf81",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Activation functions introduce non-linearity in neural networks, enabling them to learn complex mappings.\n",
    "- **Sigmoid:** Outputs in range (0,1), good for probabilities, but suffers from vanishing gradient.\n",
    "- **Tanh:** Outputs in range (-1,1), zero-centered, better than Sigmoid but still prone to vanishing gradients.\n",
    "- **ReLU:** Outputs positive values directly, efficient and widely used, but suffers from the 'dying ReLU' problem where neurons may become inactive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb25ee",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**What is the difference between Loss function and Cost function in neural networks? Provide examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd354c",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- **Loss function** measures the error for a single training example (e.g., Mean Squared Error, Binary Cross-Entropy).\n",
    "- **Cost function** is the average of loss functions across the entire training dataset.\n",
    "For example, Binary Cross-Entropy Loss for one sample vs. the overall cost computed as the mean of all individual losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15351a",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "**What is the role of optimizers in neural networks? Compare Gradient Descent, Adam, and RMSprop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88725648",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Optimizers adjust weights to minimize the loss function.\n",
    "- **Gradient Descent:** Updates weights using the slope of the loss function; can be slow.\n",
    "- **RMSprop:** Uses adaptive learning rates by normalizing gradients, good for non-stationary problems.\n",
    "- **Adam:** Combines momentum and RMSprop, efficient and widely used for training deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88b368",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**Write a Python program to implement a single-layer perceptron from scratch using NumPy to solve the logical AND gate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# AND gate data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.zeros(2)\n",
    "bias = 0\n",
    "lr = 0.1\n",
    "\n",
    "# Training\n",
    "for epoch in range(20):\n",
    "    for i in range(len(X)):\n",
    "        linear = np.dot(X[i], weights) + bias\n",
    "        y_pred = 1 if linear >= 0 else 0\n",
    "        error = y[i] - y_pred\n",
    "        weights += lr * error * X[i]\n",
    "        bias += lr * error\n",
    "\n",
    "print(\"Trained weights:\", weights)\n",
    "print(\"Trained bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8cea3",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "**Implement and visualize Sigmoid, ReLU, and Tanh activation functions using Matplotlib.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54851e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "\n",
    "sigmoid = 1/(1+np.exp(-x))\n",
    "relu = np.maximum(0,x)\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "plt.plot(x,sigmoid,label='Sigmoid')\n",
    "plt.plot(x,relu,label='ReLU')\n",
    "plt.plot(x,tanh,label='Tanh')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17877be6",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "**Use Keras to build and train a simple multilayer neural network on the MNIST digits dataset. Print the training accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# Build model\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28,28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "print(\"Training Accuracy:\", history.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c9760",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "**Visualize the loss and accuracy curves for a neural network model trained on the Fashion MNIST dataset. Interpret the training behavior.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Plot curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b19efd",
   "metadata": {},
   "source": [
    "**Interpretation:** If training and validation curves are close, the model generalizes well. A large gap indicates overfitting, while both low accuracies indicate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b561c",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "**Fraudulent transaction detection workflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fb84f",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- **Model Design:** A multilayer neural network with dense layers.\n",
    "- **Activation & Loss:** ReLU for hidden layers, Sigmoid for output; Binary Cross-Entropy loss since it's a binary classification task.\n",
    "- **Training & Evaluation:** Use stratified sampling, class weighting, or SMOTE for imbalance; evaluate using Precision, Recall, F1-score, and AUC.\n",
    "- **Optimizer & Regularization:** Adam optimizer for fast convergence; apply dropout, early stopping, and L2 regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Example model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64,\n",
    "                    validation_split=0.2, class_weight=class_weights)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
