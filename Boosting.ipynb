{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f80e61",
   "metadata": {},
   "source": [
    "# Boosting Techniques – Questions and Answers (1–10)\n",
    "Each question is followed by its answer and required Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16887f",
   "metadata": {},
   "source": [
    "### Question 1: What is Boosting in Machine Learning?\n",
    "**Answer:** Boosting is an ensemble technique that combines multiple weak learners sequentially, where each new model focuses on correcting the errors of the previous models. This improves predictive accuracy by reducing bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230db2c",
   "metadata": {},
   "source": [
    "### Question 2: Difference between AdaBoost and Gradient Boosting\n",
    "**Answer:** AdaBoost adjusts sample weights to focus on misclassified samples, while Gradient Boosting trains models sequentially by minimizing a loss function using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a455edf",
   "metadata": {},
   "source": [
    "### Question 3: How does regularization help in XGBoost?\n",
    "**Answer:** Regularization in XGBoost (L1 and L2 penalties) prevents overfitting by penalizing complex models and encouraging simpler trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431ac21",
   "metadata": {},
   "source": [
    "### Question 4: Why is CatBoost efficient for categorical data?\n",
    "**Answer:** CatBoost handles categorical variables natively using ordered target encoding, reducing preprocessing effort and preventing target leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98cec1",
   "metadata": {},
   "source": [
    "### Question 5: Real-world applications where boosting is preferred\n",
    "**Answer:** Boosting is preferred in credit risk prediction, fraud detection, recommendation systems, and ranking problems where high predictive accuracy is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dba43",
   "metadata": {},
   "source": [
    "### Question 6: AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97464da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5509a",
   "metadata": {},
   "source": [
    "### Question 7: Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(\"R2 Score:\", r2_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a983e1",
   "metadata": {},
   "source": [
    "### Question 8: XGBoost with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {'learning_rate':[0.01,0.1,0.2]}\n",
    "grid = GridSearchCV(XGBClassifier(eval_metric='logloss'), params, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best = grid.best_estimator_\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, best.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333d46c",
   "metadata": {},
   "source": [
    "### Question 9: CatBoost Classifier with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21b0e1",
   "metadata": {},
   "source": [
    "\n",
    "### Question 10: Boosting Pipeline for Loan Default Prediction\n",
    "**Answer:**  \n",
    "1. Preprocess data by imputing missing values, encoding categorical variables, and scaling numerical features if required.  \n",
    "2. Choose CatBoost if categorical features dominate, XGBoost for high performance and flexibility, and AdaBoost for simpler problems.  \n",
    "3. Tune hyperparameters using GridSearchCV or Bayesian optimization.  \n",
    "4. Evaluate using ROC-AUC, Precision-Recall, and F1-score due to class imbalance.  \n",
    "5. The business benefits from improved default prediction accuracy, better risk management, and more informed lending decisions.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
