{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616da27a",
   "metadata": {},
   "source": [
    "\n",
    "# Regression Assignment Solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: What is Simple Linear Regression?\n",
    "**Answer:**  \n",
    "Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) using a straight-line equation:\n",
    "\n",
    "Y = β0 + β1X\n",
    "\n",
    "Where:\n",
    "- β0 = Intercept\n",
    "- β1 = Slope (coefficient)\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2: What are the key assumptions of Simple Linear Regression?\n",
    "**Answer:**\n",
    "1. Linearity\n",
    "2. Independence of errors\n",
    "3. Homoscedasticity\n",
    "4. Normality of residuals\n",
    "5. No multicollinearity (for multiple regression)\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3: What is heteroscedasticity?\n",
    "**Answer:**  \n",
    "Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variable.  \n",
    "It is important because it can lead to inefficient estimates and incorrect statistical inferences.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4: What is Multiple Linear Regression?\n",
    "**Answer:**  \n",
    "Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables.\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βnXn\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5: What is Polynomial Regression?\n",
    "**Answer:**  \n",
    "Polynomial Regression models nonlinear relationships by adding polynomial terms (X², X³, etc.).  \n",
    "Unlike simple linear regression, it can fit curved relationships.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32d98f",
   "metadata": {},
   "source": [
    "## Question 6: Simple Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\n",
    "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, predictions)\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Slope:\", model.coef_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f84cdd",
   "metadata": {},
   "source": [
    "## Question 7: Multiple Linear Regression & VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Area': [1200, 1500, 1800, 2000],\n",
    "    'Rooms': [2, 3, 3, 4],\n",
    "    'Price': [250000, 300000, 320000, 370000]\n",
    "})\n",
    "\n",
    "X = data[['Area', 'Rooms']]\n",
    "y = data['Price']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19982efe",
   "metadata": {},
   "source": [
    "## Question 8: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ee743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\n",
    "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, Y)\n",
    "\n",
    "X_range = np.linspace(1,5,100).reshape(-1,1)\n",
    "X_range_poly = poly.transform(X_range)\n",
    "Y_pred = model.predict(X_range_poly)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X_range, Y_pred)\n",
    "plt.title(\"Polynomial Regression (Degree 2)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ff241",
   "metadata": {},
   "source": [
    "## Question 9: Residual Plot & Heteroscedasticity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([10, 20, 30, 40, 50]).reshape(-1,1)\n",
    "Y = np.array([15, 35, 40, 50, 65])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "predictions = model.predict(X)\n",
    "residuals = Y - predictions\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(predictions, residuals)\n",
    "plt.axhline(y=0)\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68c171",
   "metadata": {},
   "source": [
    "\n",
    "## Question 10: Handling Heteroscedasticity & Multicollinearity\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Steps to address heteroscedasticity:\n",
    "1. Apply log or Box-Cox transformation\n",
    "2. Use Weighted Least Squares\n",
    "3. Use robust standard errors\n",
    "\n",
    "Steps to address multicollinearity:\n",
    "1. Check VIF values\n",
    "2. Remove highly correlated features\n",
    "3. Use Ridge or Lasso Regression\n",
    "4. Apply PCA (Principal Component Analysis)\n",
    "\n",
    "These techniques improve model stability and predictive performance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
