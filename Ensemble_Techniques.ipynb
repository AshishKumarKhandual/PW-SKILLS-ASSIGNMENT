{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc8a958",
   "metadata": {},
   "source": [
    "# Ensemble Learning – Questions and Answers (1–10)\n",
    "Each question is followed by its answer and corresponding Python code where required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c667d9",
   "metadata": {},
   "source": [
    "### Question 1: What is Ensemble Learning?\n",
    "**Answer:** Ensemble learning is a technique that combines multiple machine learning models to produce a stronger predictive model. The key idea is that combining several weak or moderately strong learners reduces variance, bias, and improves overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5eb810",
   "metadata": {},
   "source": [
    "### Question 2: Difference between Bagging and Boosting\n",
    "**Answer:** Bagging trains models independently on different bootstrap samples and averages predictions, mainly reducing variance. Boosting trains models sequentially where each model focuses on correcting previous errors, reducing both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5355e",
   "metadata": {},
   "source": [
    "### Question 3: What is bootstrap sampling?\n",
    "**Answer:** Bootstrap sampling is sampling with replacement from the dataset to create multiple training subsets. In Bagging and Random Forest, it allows each model to be trained on a slightly different dataset, improving robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e167b",
   "metadata": {},
   "source": [
    "### Question 4: What are OOB samples?\n",
    "**Answer:** Out-of-Bag samples are the data points not selected in a bootstrap sample. These samples can be used as a validation set to estimate model performance without needing separate validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350e1cb",
   "metadata": {},
   "source": [
    "### Question 5: Feature importance: Decision Tree vs Random Forest\n",
    "**Answer:** A single Decision Tree computes importance based on splits in that tree only, making it unstable. Random Forest averages importance across many trees, producing more reliable feature importance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f43cd3",
   "metadata": {},
   "source": [
    "### Question 6: Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importance = pd.Series(model.feature_importances_, index=data.feature_names)\n",
    "print(importance.sort_values(ascending=False).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45b9da",
   "metadata": {},
   "source": [
    "### Question 7: Bagging vs Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50).fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"Bagging:\", accuracy_score(y_test, bag.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ae081",
   "metadata": {},
   "source": [
    "### Question 8: Random Forest GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "params = {'n_estimators':[50,100], 'max_depth':[None,5,10]}\n",
    "grid = GridSearchCV(RandomForestClassifier(), params, cv=3)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fef7f",
   "metadata": {},
   "source": [
    "### Question 9: Bagging vs Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c19e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bag = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50).fit(X_train, y_train)\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\n",
    "\n",
    "print(\"Bagging MSE:\", mean_squared_error(y_test, bag.predict(X_test)))\n",
    "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca52fb",
   "metadata": {},
   "source": [
    "\n",
    "### Question 10: Real-world Ensemble Strategy\n",
    "**Answer:**  \n",
    "1. Choose Bagging if variance is high and data is noisy; choose Boosting if bias is high.  \n",
    "2. Handle overfitting using cross-validation, early stopping (boosting), and limiting tree depth.  \n",
    "3. Select decision trees as base models due to flexibility.  \n",
    "4. Evaluate using K-fold cross-validation with metrics like ROC-AUC.  \n",
    "5. Ensemble learning improves decision-making by producing more stable predictions, reducing risk in financial decisions such as loan default prediction.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
