{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc94bd3",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Clustering Assignment Solutions\n",
    "## PwSkills – Java + DSA\n",
    "\n",
    "This notebook contains **all 48 questions** including:\n",
    "\n",
    "✅ Theoretical Questions (with explanations)  \n",
    "✅ Practical Questions (with Python implementations)  \n",
    "\n",
    "Topics covered:\n",
    "- Unsupervised Learning\n",
    "- K-Means Clustering\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "- Silhouette Score\n",
    "- PCA and t-SNE\n",
    "- Clustering Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad314388",
   "metadata": {},
   "source": [
    "## Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e77c8",
   "metadata": {},
   "source": [
    "### Q1. What is unsupervised learning in the context of machine learning?\n",
    "\n",
    "**Answer:**\n",
    "Unsupervised learning is a machine learning approach where models learn patterns from unlabeled data. The algorithm identifies hidden structures, similarities, or groupings without predefined outputs. Clustering and dimensionality reduction are common unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81cd9",
   "metadata": {},
   "source": [
    "### Q2. How does K-Means clustering algorithm work?\n",
    "\n",
    "**Answer:**\n",
    "K-Means divides data into K clusters by initializing K centroids, assigning data points to the nearest centroid, updating centroids as the mean of assigned points, and repeating the process until centroids stabilize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0eda",
   "metadata": {},
   "source": [
    "### Q3. Explain the concept of a dendrogram in hierarchical clustering.\n",
    "\n",
    "**Answer:**\n",
    "A dendrogram is a tree-like diagram used in hierarchical clustering that shows how clusters are merged or split at different distance levels. It helps visualize cluster relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6757df",
   "metadata": {},
   "source": [
    "### Q4. What is the main difference between K-Means and Hierarchical Clustering?\n",
    "\n",
    "**Answer:**\n",
    "K-Means requires specifying the number of clusters beforehand and partitions data iteratively, while hierarchical clustering builds a hierarchy of clusters and does not require a predefined number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b553268",
   "metadata": {},
   "source": [
    "### Q5. What are the advantages of DBSCAN over K-Means?\n",
    "\n",
    "**Answer:**\n",
    "DBSCAN can detect clusters of arbitrary shape, automatically identifies noise/outliers, and does not require specifying the number of clusters beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be71d5",
   "metadata": {},
   "source": [
    "### Q6. When would you use Silhouette Score in clustering?\n",
    "\n",
    "**Answer:**\n",
    "Silhouette Score is used to evaluate clustering performance and to select the optimal number of clusters by measuring how well each data point fits within its cluster compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e4802",
   "metadata": {},
   "source": [
    "### Q7. What are the limitations of Hierarchical Clustering?\n",
    "\n",
    "**Answer:**\n",
    "It is computationally expensive for large datasets, sensitive to noise and outliers, and once clusters are merged or split, the process cannot be reversed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cbf13",
   "metadata": {},
   "source": [
    "### Q8. Why is feature scaling important in clustering algorithms like K-Means?\n",
    "\n",
    "**Answer:**\n",
    "K-Means relies on distance calculations. Features with larger scales dominate the distance metric, leading to biased clustering results. Scaling ensures equal contribution from all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152e426",
   "metadata": {},
   "source": [
    "### Q9. How does DBSCAN identify noise points?\n",
    "\n",
    "**Answer:**\n",
    "DBSCAN labels points as noise if they do not have enough neighboring points within a specified radius (eps) to form a dense region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50fa11",
   "metadata": {},
   "source": [
    "### Q10. Define inertia in the context of K-Means.\n",
    "\n",
    "**Answer:**\n",
    "Inertia is the sum of squared distances between data points and their assigned cluster centroids. It measures cluster compactness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70aa185",
   "metadata": {},
   "source": [
    "### Q11. What is the elbow method in K-Means clustering?\n",
    "\n",
    "**Answer:**\n",
    "The elbow method plots inertia against different values of K. The point where inertia starts decreasing slowly forms an elbow, indicating an optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da289652",
   "metadata": {},
   "source": [
    "### Q12. Describe the concept of 'density' in DBSCAN.\n",
    "\n",
    "**Answer:**\n",
    "Density refers to the concentration of data points in a region. DBSCAN forms clusters in areas where the number of points within a given radius exceeds a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade30854",
   "metadata": {},
   "source": [
    "### Q13. Can hierarchical clustering be used on categorical data?\n",
    "\n",
    "**Answer:**\n",
    "Yes, hierarchical clustering can be used with categorical data if appropriate similarity or distance measures such as Hamming distance are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c30142",
   "metadata": {},
   "source": [
    "### Q14. What does a negative Silhouette Score indicate?\n",
    "\n",
    "**Answer:**\n",
    "A negative Silhouette Score indicates that a data point may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b71f26",
   "metadata": {},
   "source": [
    "### Q15. Explain the term 'linkage criteria' in hierarchical clustering.\n",
    "\n",
    "**Answer:**\n",
    "Linkage criteria determine how distances between clusters are computed, such as single linkage, complete linkage, average linkage, and Ward linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1946a",
   "metadata": {},
   "source": [
    "### Q16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
    "\n",
    "**Answer:**\n",
    "K-Means assumes clusters are spherical and similar in size. When clusters vary significantly in size or density, centroids may be incorrectly positioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f2d81",
   "metadata": {},
   "source": [
    "### Q17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
    "\n",
    "**Answer:**\n",
    "The core parameters are eps (radius defining neighborhood size) and min_samples (minimum number of points needed to form a dense region). These parameters control cluster formation and noise detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e4caf",
   "metadata": {},
   "source": [
    "### Q18. How does K-Means++ improve upon standard K-Means initialization?\n",
    "\n",
    "**Answer:**\n",
    "K-Means++ selects initial centroids that are far apart from each other, improving convergence speed and reducing the chances of poor clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c57369",
   "metadata": {},
   "source": [
    "### Q19. What is agglomerative clustering?\n",
    "\n",
    "**Answer:**\n",
    "A bottom-up hierarchical clustering method where each data point starts as its own cluster and clusters are iteratively merged based on similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8b054",
   "metadata": {},
   "source": [
    "### Q20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
    "\n",
    "**Answer:**\n",
    "Inertia measures only compactness, while Silhouette Score considers both intra-cluster cohesion and inter-cluster separation, making it a better evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52077d20",
   "metadata": {},
   "source": [
    "## Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.datasets import load_iris, load_wine, load_digits, load_breast_cancer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q21\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=labels)\n",
    "plt.title(\"KMeans with 4 centers\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q22\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "labels = agg.fit_predict(X)\n",
    "print(labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ebe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q23\n",
    "X,_ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=labels)\n",
    "plt.title(\"DBSCAN - make_moons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q24\n",
    "wine = load_wine()\n",
    "X = StandardScaler().fit_transform(wine.data)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q25\n",
    "X,_ = make_circles(n_samples=300, noise=0.05, factor=0.5)\n",
    "labels = DBSCAN(eps=0.2).fit_predict(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=labels)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
